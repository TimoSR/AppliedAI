{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def h(theta, x):\n",
    "    return theta[0] + theta[1] * x\n",
    "\n",
    "def gradient_step(theta, x, y, alpha, verbose=False):\n",
    "    if verbose: print(\"Gradient step \", theta, x, y, alpha)\n",
    "    delta = np.zeros(np.shape(theta))\n",
    "    m = len(y)\n",
    "    for i in range(m):\n",
    "        delta[0] -= (2/float(m)) * (y[i] - h(theta, x[i]))\n",
    "        delta[1] -= (2/float(m)) * (y[i] - h(theta, x[i])) * x[i]\n",
    "        if verbose: print(i, delta)\n",
    "    if verbose:\n",
    "        print(\"Theta\", theta - alpha * delta)\n",
    "        print(\"Cost\", sum(1/(2*m) * np.square(h(theta, np.array(x)) - np.array(y))))\n",
    "    return theta - alpha * delta\n",
    "\n",
    "def gradient_descent(x, y, initial_theta, alpha, iterations, verbose=False):\n",
    "    theta = initial_theta\n",
    "    for i in range(iterations):\n",
    "        if verbose: print(\"** Iteration \", i)\n",
    "        theta = gradient_step(theta, x, y, alpha, verbose)\n",
    "    return theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x and y is the data set\n",
    "\n",
    "The theta value we calculated gives us a hypothesis function h.\n",
    "\n",
    "initial_theta is the value that we use for theta to begin with\n",
    "\n",
    "alpha is the learning rate\n",
    "\n",
    "iterations is the number of times we are using the data set to optimize theta, and finally verbose lets us get a lot of output during processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.31521739, 0.19565217])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 3, 4.5, 5.5]\n",
    "y = [2.5, 3, 3, 3.5]\n",
    "\n",
    "# Setting Hyper Paramaters\n",
    "\n",
    "initial_theta = np.array([1, 2])\n",
    "alpha = 0.06\n",
    "iterations = 2000\n",
    "\n",
    "gradient_descent(x, \n",
    "                 y, \n",
    "                 initial_theta, \n",
    "                 alpha, \n",
    "                 iterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Has both the math and the more user friendly naming.\n",
    "\n",
    "It supports 2 different target groups beginners and experts, as it support different ways of accessing the same thing.\n",
    "\n",
    "This enables the code to addapt to the language of the field.\n",
    "\n",
    "The code is slightly more modular, with separate functions for calculating the cost and optimizing the weights. This structure can be beneficial if you want to reuse these functions elsewhere in your code or expand the functionality of the linear regression model. The second code also uses more descriptive function names, which can help in understanding the code's purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [ 3.99999813 -0.99999918]\n",
      "Cost after optimization: 2.5083021140820744e-13\n",
      "Optimized weights: [ 3.99999813 -0.99999918]\n",
      "Cost after optimization: 2.5083021140820744e-13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Input\n",
    "\n",
    "dataset_x = np.array([1, 2, 3])\n",
    "dataset_y = np.array([3, 2, 1])\n",
    "\n",
    "# Hyper Paramaters\n",
    "\n",
    "initial_hypothesis = np.array([1, 2])\n",
    "learning_rate = 0.06\n",
    "loop_iterations = 2000\n",
    "\n",
    "def calculate_cost_linear(x_data, y_data, weights):\n",
    "    return compute_cost_lin(x=x_data, y=y_data, theta=weights)\n",
    "\n",
    "def optimize_weights_linear(x_data, y_data, weights, learning_rate, iterations):\n",
    "    return gradient_descent_lin(x=x_data, y=y_data, theta=weights, alpha=learning_rate, iterations=iterations)\n",
    "\n",
    "def compute_cost_lin(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost for linear regression.\n",
    "\n",
    "    :param x: input data for x\n",
    "    :param y: input data for y\n",
    "    :param theta: optimized theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    x = x.reshape(-1, 1)\n",
    "    x = np.hstack((np.ones_like(x), x))  # Add a column of ones for the bias term\n",
    "    predictions = x.dot(theta)\n",
    "    error = np.sum((predictions - y) ** 2) / (2 * m)\n",
    "    return error\n",
    "\n",
    "def gradient_descent_lin(x, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    This function performs the gradient descent calculation.\n",
    "\n",
    "    :param x: input data for x\n",
    "    :param y: input data for y\n",
    "    :param theta: initial hypothesis (theta values)\n",
    "    :param alpha: learning rate\n",
    "    :param iterations: number of iterations to perform\n",
    "    :return: optimized theta values after gradient descent\n",
    "    \"\"\"\n",
    "    m = len(y) # Length of array\n",
    "    x = x.reshape(-1, 1)\n",
    "    x = np.hstack((np.ones_like(x), x))  # Add a column of ones for the bias term\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = x.dot(theta)\n",
    "        error = predictions - y\n",
    "        gradient = (1 / m) * x.T.dot(error)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "# Beginner Approach\n",
    "\n",
    "# Perform gradient descent and print the results\n",
    "optimized_weights = optimize_weights_linear(x_data=dataset_x, \n",
    "                                            y_data=dataset_y, \n",
    "                                            weights=initial_hypothesis, \n",
    "                                            learning_rate=learning_rate, \n",
    "                                            iterations=loop_iterations)\n",
    "\n",
    "print(\"Optimized weights:\", optimized_weights)\n",
    "\n",
    "print(\"Cost after optimization:\", calculate_cost_linear(x_data=dataset_x, \n",
    "                                                        y_data=dataset_y, \n",
    "                                                        weights=optimized_weights))\n",
    "\n",
    "# Mathmetical Approach\n",
    "\n",
    "# Perform gradient descent and print the results\n",
    "optimized_theta = gradient_descent_lin(x=dataset_x, \n",
    "                                       y=dataset_y, \n",
    "                                       theta=initial_hypothesis, \n",
    "                                       alpha=learning_rate, \n",
    "                                       iterations=loop_iterations)\n",
    "\n",
    "print(\"Optimized weights:\", optimized_weights)\n",
    "\n",
    "print(\"Cost after optimization:\", compute_cost_lin(x=dataset_x, \n",
    "                                                   y=dataset_y, \n",
    "                                                   theta=optimized_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
