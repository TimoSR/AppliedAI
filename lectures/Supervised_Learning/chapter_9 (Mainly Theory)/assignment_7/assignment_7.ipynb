{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def h(theta, x):\n",
    "    return theta[0] + theta[1] * x\n",
    "\n",
    "def gradient_step(theta, x, y, alpha, verbose=False):\n",
    "    if verbose: print(\"Gradient step \", theta, x, y, alpha)\n",
    "    delta = np.zeros(np.shape(theta))\n",
    "    m = len(y)\n",
    "    for i in range(m):\n",
    "        delta[0] -= (2/float(m)) * (y[i] - h(theta, x[i]))\n",
    "        delta[1] -= (2/float(m)) * (y[i] - h(theta, x[i])) * x[i]\n",
    "        if verbose: print(i, delta)\n",
    "    if verbose:\n",
    "        print(\"Theta\", theta - alpha * delta)\n",
    "        print(\"Cost\", sum(1/(2*m) * np.square(h(theta, np.array(x)) - np.array(y))))\n",
    "    return theta - alpha * delta\n",
    "\n",
    "def gradient_descent(x, y, initial_theta, alpha, iterations, verbose=False):\n",
    "    theta = initial_theta\n",
    "    for i in range(iterations):\n",
    "        if verbose: print(\"** Iteration \", i)\n",
    "        theta = gradient_step(theta, x, y, alpha, verbose)\n",
    "    return theta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x and y is the data set\n",
    "\n",
    "The theta value we calculated gives us a hypothesis function h.\n",
    "\n",
    "initial_theta is the value that we use for theta to begin with\n",
    "\n",
    "alpha is the learning rate\n",
    "\n",
    "iterations is the number of times we are using the data set to optimize theta, and finally verbose lets us get a lot of output during processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.31521739, 0.19565217])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 3, 4.5, 5.5]\n",
    "y = [2.5, 3, 3, 3.5]\n",
    "\n",
    "# Setting Hyper Paramaters\n",
    "\n",
    "initial_theta = np.array([1, 2])\n",
    "alpha = 0.06\n",
    "iterations = 2000\n",
    "\n",
    "gradient_descent(x, \n",
    "                 y, \n",
    "                 initial_theta, \n",
    "                 alpha, \n",
    "                 iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized theta values: [ 3.99999813 -0.99999918]\n",
      "Cost after optimization: 2.5083021140820744e-13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data Input\n",
    "\n",
    "dataset_x = np.array([1, 2, 3])\n",
    "dataset_y = np.array([3, 2, 1])\n",
    "\n",
    "# Hyper Paramaters\n",
    "\n",
    "initial_hypothesis = np.array([1, 2])\n",
    "learning_rate = 0.06\n",
    "loop_iterations = 2000\n",
    "\n",
    "def compute_cost_lin(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the cost for linear regression.\n",
    "\n",
    "    :param x: input data for x\n",
    "    :param y: input data for y\n",
    "    :param theta: optimized theta\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    x = x.reshape(-1, 1)\n",
    "    x = np.hstack((np.ones_like(x), x))  # Add a column of ones for the bias term\n",
    "    predictions = x.dot(theta)\n",
    "    error = np.sum((predictions - y) ** 2) / (2 * m)\n",
    "    return error\n",
    "\n",
    "def gradient_descent_lin(x, y, theta, alpha, iterations):\n",
    "    \"\"\"\n",
    "    This function performs the gradient descent calculation.\n",
    "\n",
    "    :param x: input data for x\n",
    "    :param y: input data for y\n",
    "    :param theta: initial hypothesis (theta values)\n",
    "    :param alpha: learning rate\n",
    "    :param iterations: number of iterations to perform\n",
    "    :return: optimized theta values after gradient descent\n",
    "    \"\"\"\n",
    "    m = len(y)\n",
    "    x = x.reshape(-1, 1)\n",
    "    x = np.hstack((np.ones_like(x), x))  # Add a column of ones for the bias term\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = x.dot(theta)\n",
    "        error = predictions - y\n",
    "        gradient = (1 / m) * x.T.dot(error)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "# Execute the gradient descent function and print the result using keyword arguments\n",
    "optimized_theta = gradient_descent_lin(x=dataset_x, \n",
    "                                       y=dataset_y, \n",
    "                                       theta=initial_hypothesis, \n",
    "                                       alpha=learning_rate, \n",
    "                                       iterations=loop_iterations)\n",
    "\n",
    "print(\"Optimized theta values:\", optimized_theta)\n",
    "\n",
    "print(\"Cost after optimization:\", compute_cost_lin(x=dataset_x, \n",
    "                                                   y=dataset_y, \n",
    "                                                   theta=optimized_theta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier to read version of the code optimized by chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized weights: [ 3.99999813 -0.99999918]\n",
      "Cost after optimization: 2.5083021140820744e-13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "x_values = np.array([1, 2, 3])\n",
    "y_values = np.array([3, 2, 1])\n",
    "\n",
    "# Hyperparameters\n",
    "initial_weights = np.array([1, 2])\n",
    "learning_rate = 0.06\n",
    "iterations = 2000\n",
    "\n",
    "def calculate_cost_linear(x_data, y_data, weights):\n",
    "    \"\"\"\n",
    "    Calculate the cost for a linear regression model.\n",
    "\n",
    "    :param x_data: the input data for x\n",
    "    :param y_data: the input data for y\n",
    "    :param weights: the optimized weights for the model\n",
    "    \"\"\"\n",
    "    num_samples = len(y_data)\n",
    "    x_data = x_data.reshape(-1, 1)\n",
    "    x_data = np.hstack((np.ones_like(x_data), x_data))  # Add a column of ones for the bias term\n",
    "    predictions = x_data.dot(weights)\n",
    "    error = np.sum((predictions - y_data) ** 2) / (2 * num_samples)\n",
    "    return error\n",
    "\n",
    "def optimize_weights_linear(x_data, y_data, weights, learning_rate, iterations):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to optimize the weights for a linear regression model.\n",
    "\n",
    "    :param x_data: the input data for x\n",
    "    :param y_data: the input data for y\n",
    "    :param weights: the initial weights for the model\n",
    "    :param learning_rate: the learning rate for the optimization algorithm\n",
    "    :param iterations: the number of iterations to perform\n",
    "    :return: the optimized weights for the model\n",
    "    \"\"\"\n",
    "    number_samples = len(y_data)\n",
    "    x_data = x_data.reshape(-1, 1)\n",
    "    x_data = np.hstack((np.ones_like(x_data), x_data))  # Add a column of ones for the bias term\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = x_data.dot(weights)\n",
    "        error = predictions - y_data\n",
    "        gradient = (1 / number_samples) * x_data.T.dot(error)\n",
    "        weights = weights - learning_rate * gradient\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Perform gradient descent and print the results\n",
    "optimized_weights = optimize_weights_linear(x_data=x_values, \n",
    "                                            y_data=y_values, \n",
    "                                            weights=initial_weights, \n",
    "                                            learning_rate=learning_rate, \n",
    "                                            iterations=iterations)\n",
    "\n",
    "print(\"Optimized weights:\", optimized_weights)\n",
    "\n",
    "print(\"Cost after optimization:\", calculate_cost_linear(x_data=x_values, \n",
    "                                                        y_data=y_values, \n",
    "                                                        weights=optimized_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-Mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
